{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR programming #3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMCrzxHGFsLK"
      },
      "source": [
        "**Informaiton Retrieval Programming Assignment #3**\n",
        "<br>**Real-world Indexing System(final part)**\n",
        "<br>Build a simple but true to practice retrieval engine based on the cosine similarity vector space model\n",
        "<br>- Build index of file(dictionary + inverted file) for a 360 MB sized test collection of biomedical articles related to COVID-19\n",
        "<br>- Inverted file is written to disk as a binary file\n",
        "<br>- Use TF/IDF term weighting for both documents and the query\n",
        "<br>- Compute cosine similarity for all documents in collection\n",
        "<br>- Have a query processing program to load the dictionary and retrive posting lists for terms from inverted file in order to rank documents for provided set of queries\n",
        "<br>- Run a comparision test of whether longer queries lead to more accurate retrieval results\n",
        "\n",
        "<br><br>**Author:** Helen Ting He; **Date:** Oct 16, 2021"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxDIcpNKdv_1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "28bc7585-6ef9-43bc-9e4a-84809ae296f3"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "import string #remove punctuation\n",
        "from collections import Counter, defaultdict, OrderedDict\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import math\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "mXyNVXTB-6aO",
        "outputId": "4e8be910-0627-4531-d019-ea76df1b8f59"
      },
      "source": [
        "# save to google colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/' "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RqYAkIvd-UB"
      },
      "source": [
        "# read original files\n",
        "with open('cord19.txt') as f:\n",
        "    lines = f.readlines()\n",
        "cord_file = pd.DataFrame(list(zip(lines)))\n",
        "\n",
        "cord_key_url = 'http://pmcnamee.net/744/data/cord19.topics.keyword.txt'\n",
        "cord_key = pd.read_csv(cord_key_url,sep='\\n',header=None)\n",
        "\n",
        "cord_qes_url = 'http://pmcnamee.net/744/data/cord19.topics.question.txt'\n",
        "cord_qes = pd.read_csv(cord_qes_url,sep='\\n',header=None)\n",
        "\n",
        "animal_url = 'http://pmcnamee.net/744/data/animal.txt'\n",
        "animal = pd.read_csv(animal_url,sep='\\n',header=None)\n",
        "\n",
        "ani_top_url = 'http://pmcnamee.net/744/data/animal.topics.txt'\n",
        "ani_top = pd.read_csv(ani_top_url,sep='\\n',header=None)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_CkhBoa0ocX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "f7d1c0bc-54f0-4391-c06b-ae7ae4a8e8ca"
      },
      "source": [
        "# Test files with 3000 documents \n",
        "with open('cord19_test.txt') as f:\n",
        "    lines = f.readlines()\n",
        "test_file = pd.DataFrame(list(zip(lines)))\n",
        "\n",
        "with open('animal.txt') as f:\n",
        "    lines = f.readlines()\n",
        "test_animal_file = pd.DataFrame(list(zip(lines)))\n",
        "\n",
        "with open('animal.topics.txt') as f:\n",
        "    lines = f.readlines()\n",
        "test_animal_query = pd.DataFrame(list(zip(lines)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"with open('animal.txt') as f:\\n    lines = f.readlines()\\ntest_animal_file = pd.DataFrame(list(zip(lines)))\\n\\nwith open('animal.topics.txt') as f:\\n    lines = f.readlines()\\ntest_animal_query = pd.DataFrame(list(zip(lines)))\""
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "DC4Vw5nRv7ln",
        "outputId": "f234e54d-2205-4152-bb85-9ed56a760a81"
      },
      "source": [
        "# Num of docs in file\n",
        "substring = \"<P ID=\"\n",
        "substring_q = \"<Q ID=\"\n",
        "def num_doc(data_frame):\n",
        "  doc = 0\n",
        "  for sent_i in range(len(data_frame[0])):\n",
        "    if substring in data_frame[0][sent_i] or substring_q in data_frame[0][sent_i]:\n",
        "      doc += 1\n",
        "  return doc\n",
        "print(\"cord_19 dataset has \" + str(num_doc(cord_file)) + \" documents\")\n",
        "print(\"cord_key dataset has \" + str(num_doc(cord_key)) + \" documents\")\n",
        "print(\"cord_key (full) dataset has \" + str(num_doc(cord_qes)) + \" documents\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cord_19 dataset has 191175 documents\n",
            "cord_key dataset has 50 documents\n",
            "cord_key (full) dataset has 50 documents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iLWZdXNosLH"
      },
      "source": [
        "#######################################\n",
        "# Initilization\n",
        "# contains 1) normalization\n",
        "# 2) create posing list \n",
        "# 3) create index (dictionary and inverted file)\n",
        "#######################################\n",
        "\n",
        "def normalization(each_para):\n",
        "  # To normalize each paragraph\n",
        "  # @input: each phragraph, string\n",
        "  # @output: counter of word for each paragraph, dict\n",
        "  # normalized method:\n",
        "  # 1. lower case \n",
        "  # 2. complete notation of apostrophe\n",
        "  # 3. lemmenization using wordnetLemmatizer()\n",
        "  # 4. remove word contain non-alphabetic character (except letter, number, underscore)\n",
        "  output_list_para = []\n",
        "  counter_list_para = {}\n",
        "  each_para = each_para.lower()\n",
        "  each_para = each_para.replace(\"'re\",' are').replace(\"'s\",' is').replace(\"'ll\",' will').replace(\"n't\",' not')\n",
        "  each_para = each_para.replace(\"'\",'')\n",
        "  each_para = each_para.translate(str.maketrans('','',string.punctuation))\n",
        "  each_para = each_para.replace(\"â€“\", \"\")\n",
        "  each_para = each_para.replace(\"-\", \"_\")\n",
        "  lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "  tokens = each_para.strip().split()\n",
        "  clean_tokens = [t for t in tokens if re.match(r'[^\\W\\d]*$', t)]\n",
        "  for word in clean_tokens:\n",
        "    word = lem.lemmatize(word)\n",
        "    output_list_para.append(word)   \n",
        "    counter_list_para = Counter(output_list_para)\n",
        "  return counter_list_para\n",
        "\n",
        "def readFile(whole_text): \n",
        "  # To read the text and process relevant methods\n",
        "  # @input: input raw text, dataframe\n",
        "  # @output: paragraph one by one, string\n",
        "  i = 0\n",
        "  substring1 = \"<P ID=\"\n",
        "  substring2 = \"</P>\"\n",
        "  substring1_q = \"<Q ID=\"\n",
        "  substring2_q = \"</Q>\"\n",
        "  len_file = len(whole_text)\n",
        "  output_wordlist_dict = {}\n",
        "  for sent_i in range(len(whole_text)):\n",
        "    if substring1 in whole_text[0][sent_i] or substring1_q in whole_text[0][sent_i]:\n",
        "      para = \"\"\n",
        "    if (substring1 not in whole_text[0][sent_i] and substring2 not in whole_text[0][sent_i]) and (substring1_q not in whole_text[0][sent_i] and substring2_q not in whole_text[0][sent_i]):\n",
        "      para = para + whole_text[0][sent_i]\n",
        "    if substring2 in whole_text[0][sent_i] or substring2_q in whole_text[0][sent_i]:\n",
        "      output_wordlist_dict[i] = normalization(para)\n",
        "      i = i + 1\n",
        "  return output_wordlist_dict\n",
        "\n",
        "def posting_list(docid_counter):\n",
        "  # convert into posting list\n",
        "  # @input:dict{ [docid]:dict{[word]: word freq per document} }, dict of dict\n",
        "  # @ouput:dict{ [word]:dict{[docid]: term count} }, dict of dict\n",
        "  posting_list_output = defaultdict(lambda: Counter([]))\n",
        "  for doc, cnt in docid_counter.items():\n",
        "    for word, word_cnt in cnt.items():\n",
        "      posting_list_output[word][doc] += word_cnt\n",
        "  return posting_list_output\n",
        "\n",
        "def dictionary(posting_list):\n",
        "  # To generate the unique sorted vocabulary list as key\n",
        "  # DF and sorted offset as value \n",
        "  # @input: dict{ [word]:dict{[docid]: term count} }, dict of dict\n",
        "  # @output: sorted unique vocabulary dict with DF and offset \n",
        "  sort_dict = {}\n",
        "  result_sort_dict = {}\n",
        "  offset_sum = 0\n",
        "  offset_i = 0\n",
        "  od = OrderedDict(sorted(posting_list.items()))\n",
        "  sort_dict = OrderedDict(od)\n",
        "  for i, word_i in enumerate(sort_dict.keys()):\n",
        "    offset_i = len(sort_dict[word_i]) * 2 #offset \n",
        "    result_sort_dict[word_i] = len(sort_dict[word_i].values()),offset_sum #DF\n",
        "    offset_sum = offset_sum + offset_i \n",
        "  return result_sort_dict\n",
        "\n",
        "def inverted_file(key,posting_list):\n",
        "  # To generate inverted file\n",
        "  # @input: sorted key, list\n",
        "  #         posting list, dict of dict\n",
        "  # @output: inverted file, list\n",
        "  inverted_list = []\n",
        "  for word_i in key:\n",
        "    for docid, cnt in posting_list[word_i].items():\n",
        "      inverted_list.append(docid)\n",
        "      inverted_list.append(cnt)\n",
        "  return inverted_list"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWNBRFC2AFJP"
      },
      "source": [
        "#######################################\n",
        "# Cacualate weighting\n",
        "# contain (1)IDF for corpus\n",
        "# (2)TF-IDF for each document and query\n",
        "# (3)length of each document or query\n",
        "#######################################\n",
        "def idf_corpus(dict_corpus,original_file):\n",
        "  # To caculate IDF for corpus\n",
        "  # @input: dictionary file of corpus\n",
        "  #         original file\n",
        "  # @output: dictionary (word as key, IDF as value)\n",
        "  idf_dict = {}\n",
        "  N_corpus = num_doc(original_file)\n",
        "  for key_i in dict_corpus.keys():\n",
        "    tf_i = dict_corpus[key_i][0]\n",
        "    idf_i = math.log(N_corpus/tf_i,2)\n",
        "    idf_dict[key_i] = idf_i\n",
        "  return idf_dict\n",
        "\n",
        "def tf_idf(post_list,idf_matrix):\n",
        "  # To caculate TF-IDF for word\n",
        "  # @input: posting list of doc\n",
        "  #         idf of corpus\n",
        "  # @output: dictionary(docid as key: word as key: TF-IDF)\n",
        "  weight_matrix = defaultdict(lambda: defaultdict(float))\n",
        "  for doc_i in post_list.keys():\n",
        "    for word_i in post_list[doc_i].keys():\n",
        "      if word_i not in idf_matrix:\n",
        "        temp = 0.0\n",
        "      else:\n",
        "        temp = idf_matrix[word_i];\n",
        "      weight_i = post_list[doc_i][word_i] * temp\n",
        "      weight_matrix[doc_i][word_i] = weight_i\n",
        "  return weight_matrix\n",
        "\n",
        "def length_M(weight):\n",
        "  # To caculate length of each doc/query\n",
        "  # @input: dict of TF-IDF\n",
        "  # @output: dictionary(doci as key: length as value)\n",
        "  length_matrix = defaultdict(float)\n",
        "  for doc_i in weight.keys():\n",
        "    voc_weight_square_sum = 0.0\n",
        "    voc_weight_sum = 0.0\n",
        "    for word_i in weight[doc_i].keys():\n",
        "      if weight[doc_i][word_i] !=0.0:\n",
        "        voc_weight_square_sum = math.pow(weight[doc_i][word_i],2) + voc_weight_square_sum\n",
        "    voc_weight_sum = math.sqrt(voc_weight_square_sum)\n",
        "    length_matrix[doc_i] = voc_weight_sum\n",
        "  return length_matrix\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zq05WijzOTf-"
      },
      "source": [
        "#######################################\n",
        "# Cosine Similarity\n",
        "#######################################\n",
        "\n",
        "def cos_sim_pre(original_file, weight_query,weight_corpus,dict_corpus,invertd_corpus,length_corpus, threshold):\n",
        "  # To caculate cos similarity for each query\n",
        "  # @input: original file of corpus\n",
        "  #         weight matrix of query\n",
        "  #         weight matrix of corpus\n",
        "  #         dictionary of corpus\n",
        "  #         inverted file of corpus\n",
        "  #         length of corpus\n",
        "  #         threshold: decide which weight will be calculate\n",
        "  # @output: cos_sim for each query and each doc\n",
        "  temp = defaultdict(lambda: defaultdict(lambda:defaultdict(float)))\n",
        "  for query_i in weight_query.keys():\n",
        "    for query_word_i in dict_corpus.keys():\n",
        "      if weight_query[query_i][query_word_i] > threshold: \n",
        "        df_word_i = dict_corpus[query_word_i][0]\n",
        "        offset_word_i = dict_corpus[query_word_i][1]\n",
        "        for invert_i in range(offset_word_i, offset_word_i + df_word_i * 2,2):\n",
        "          temp[query_i][invertd_corpus[invert_i]][query_word_i] = weight_query[query_i][query_word_i] * weight_corpus[invertd_corpus[invert_i]][query_word_i]\n",
        "  return temp\n",
        "\n",
        "def cos_sim(original_file, weight_query,weight_corpus,dict_corpus,invertd_corpus,length_corpus, threshold):\n",
        "  cos_sim_M = defaultdict(lambda: defaultdict(float))\n",
        "  temp = cos_sim_pre(original_file, weight_query,weight_corpus,dict_corpus,invertd_corpus,length_corpus, threshold)\n",
        "  for query_i in weight_query.keys():\n",
        "    for doc_i in range(num_doc(original_file)):\n",
        "      temp_sum = 0\n",
        "      for word_j in temp[query_i][doc_i]:        \n",
        "        temp_sum = temp_sum + temp[query_i][doc_i][word_j]\n",
        "        cos_sim_M[query_i][doc_i] = 0.0\n",
        "        if length_corpus[doc_i]!=0.0:\n",
        "          cos_sim_M[query_i][doc_i] = temp_sum/(length_corpus[doc_i])\n",
        "  return cos_sim_M\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoLEMjTJGEnl"
      },
      "source": [
        "#######################################\n",
        "# Ranked List\n",
        "#######################################\n",
        "\n",
        "def rank_list(cos_matrix):\n",
        "  rank_file = []\n",
        "  temp = {}\n",
        "  for query_i in cos_matrix.keys():\n",
        "    temp = sorted(cos_matrix[query_i].items(), key = lambda x: x[1], reverse= True)\n",
        "    if len(temp) > 100:\n",
        "      for i in range(100):\n",
        "        row = []\n",
        "        row.append(query_i+1)\n",
        "        row.append(\"Q0\")\n",
        "        row.append(temp[i][0])\n",
        "        row.append(i+1)\n",
        "        row.append(temp[i][1])\n",
        "        row.append('the14' )\n",
        "        rank_file.append(row)\n",
        "  return rank_file\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGXDziojZGVB"
      },
      "source": [
        "temp = sorted(cos_M_covid_key[0].items(), key = lambda x: x[1], reverse= True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "KktziZ5gZP_r",
        "outputId": "98680e5d-9a32-41f7-ba2f-301b51abf90d"
      },
      "source": [
        "for i in range(50):\n",
        "  temp = sorted(cos_M_covid_key[i].items(), key = lambda x: x[1], reverse= True)\n",
        "  if len(temp) == 26:\n",
        "    print(i)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jW9YgyWTN83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "7ffc141b-9c6f-4164-9d1d-7b7144605682"
      },
      "source": [
        "#######################################\n",
        "# Main Function (part 1)\n",
        "# run shorter queries\n",
        "# (1)create index\n",
        "#######################################\n",
        "# create index for query\n",
        "start_norm = time.time()\n",
        "normal_query_key = readFile(cord_key) #nomarlized query dataset\n",
        "print(\"--- %s seconds to normalize query key----\"% (time.time() - start_norm))\n",
        "\n",
        "#posting list\n",
        "start0 = time.time()\n",
        "postlist_query_key_result = posting_list(normal_query_key) \n",
        "print(\"--- %s seconds to buid posting list for query key----\"% (time.time() - start0))\n",
        "\n",
        "#dictionary\n",
        "start1 = time.time()\n",
        "dict_query_key_output = dictionary(postlist_query_key_result) \n",
        "print(\"cord_key dataset has \" + str(len(dict_query_key_output)) + \" unique vocabulary set\")\n",
        "print(\"--- %s seconds to buid dictionary for query key----\"% (time.time() - start1))\n",
        "#save dictionary\n",
        "with open('gdrive/My Drive/dictionary_covid_query_key.txt', 'w') as f: \n",
        "    f.write(json.dumps(dict_query_key_output))\n",
        "\n",
        "#inverted file\n",
        "start2 = time.time()\n",
        "inverted_query_key = inverted_file(dict_query_key_output.keys(),postlist_query_key_result)\n",
        "print(\"--- %s seconds to buid inverted file for query key----\"% (time.time() - start2))\n",
        "# save inverted file\n",
        "with open(\"gdrive/My Drive/inverted_file_covid_query_key.bin\", \"wb\") as fb:\n",
        "  for num in inverted_query_key:\n",
        "    fb.write(num.to_bytes(4, \"big\"))\n",
        "\n",
        "########################\n",
        "# create index for covid\n",
        "start_norm_file = time.time()\n",
        "normal_file = readFile(cord_file) #nomarlized query dataset\n",
        "print(\"--- %s seconds to normalize covid dataset----\"% (time.time() - start_norm_file))\n",
        "\n",
        "#posting list\n",
        "start3 = time.time()\n",
        "postlist_result = posting_list(normal_file) \n",
        "print(\"--- %s seconds to buid posting list for covid dataset----\"% (time.time() - start3))\n",
        "\n",
        "#dictionary\n",
        "start4 = time.time()\n",
        "dict_output = dictionary(postlist_result) \n",
        "print(\"covid dataset has \" + str(len(dict_output)) + \" unique vocabulary set\")\n",
        "print(\"--- %s seconds to buid dictionary for covid dataset----\"% (time.time() - start4))\n",
        "#save dictionary\n",
        "with open('gdrive/My Drive/dictionary_covid.txt', 'w') as f: \n",
        "    f.write(json.dumps(dict_output))\n",
        "\n",
        "#inverted file\n",
        "start5 = time.time()\n",
        "covid_inverted_file= inverted_file(dict_output.keys(),postlist_result)\n",
        "print(\"--- %s seconds to buid inverted file for covid dataset---\"% (time.time() - start5))\n",
        "# save inverted file\n",
        "with open(\"gdrive/My Drive/inverted_file_covid.bin\", \"wb\") as fb:\n",
        "  for num in covid_inverted_file:\n",
        "    fb.write(num.to_bytes(4, \"big\"))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1.5164504051208496 seconds to normalize query key----\n",
            "--- 0.0004076957702636719 seconds to buid posting list for query key----\n",
            "cord_key dataset has 98 unique vocabulary set\n",
            "--- 0.00023055076599121094 seconds to buid dictionary for query key----\n",
            "--- 0.000186920166015625 seconds to buid inverted file for query key----\n",
            "--- 1667.0581352710724 seconds to normalize covid dataset----\n",
            "--- 21.818220138549805 seconds to buid posting list for covid dataset----\n",
            "covid dataset has 404445 unique vocabulary set\n",
            "--- 3.108849287033081 seconds to buid dictionary for covid dataset----\n",
            "--- 5.089956283569336 seconds to buid inverted file for covid dataset---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6qUXahD3a6z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a28bbcb8-7f00-4821-e381-511acdc9741a"
      },
      "source": [
        "#######################################\n",
        "# Main Function (part 1)\n",
        "# run shorter queries\n",
        "# (2)caculate weighting\n",
        "#######################################\n",
        "\n",
        "#####################\n",
        "# caculate weight\n",
        "start6 = time.time()\n",
        "idf_matrix = idf_corpus(dict_output,cord_file)\n",
        "covid_weight = tf_idf(normal_file,idf_matrix)\n",
        "# length of corpus\n",
        "length_covid = length_M(covid_weight)\n",
        "\n",
        "# query(short)\n",
        "covid_query_key_weight = tf_idf(normal_query_key,idf_matrix)\n",
        "length_covid_key_query = length_M(covid_query_key_weight)\n",
        "# TF-IDF for first query and doc\n",
        "print(\"weight(TF-IDF) for first query of covid_key dataset:\", \"\\n\", covid_query_key_weight[0])\n",
        "print(\"weight(TF-IDF) for first doc of covid dataset:\", \"\\n\", covid_weight[0])\n",
        "print(\"--- %s seconds to parse query including caculate IDF over corpus---\"% (time.time() - start6 ))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weight(TF-IDF) for first query of covid_key dataset: \n",
            " defaultdict(<class 'float'>, {'coronavirus': 2.0378548589196765, 'origin': 5.312113421342007})\n",
            "weight(TF-IDF) for first doc of covid dataset: \n",
            " defaultdict(<class 'float'>, {'clinical': 6.649261361053281, 'feature': 12.176869732844686, 'of': 4.800052306944922, 'cultureproven': 42.66781876089216, 'mycoplasma': 30.59886634084056, 'pneumoniae': 51.21842883572701, 'infection': 24.710641457969302, 'at': 6.4621211341642475, 'king': 26.946875772891033, 'abdulaziz': 37.91293125872869, 'university': 15.802612827662887, 'hospital': 13.183165588455166, 'jeddah': 31.566499606468884, 'saudi': 28.477273780874796, 'arabia': 28.765550092080005, 'objective': 3.3861613422893524, 'this': 4.928699874612593, 'retrospective': 4.9446215063309555, 'chart': 15.855971009478187, 'review': 2.978599270977105, 'describes': 5.690445168647509, 'the': 4.069357622991405, 'epidemiology': 10.431157973473505, 'and': 5.498784773411431, 'patient': 17.51172596207369, 'with': 7.900480644571672, 'method': 1.966604016204771, 'positive': 3.569223730461351, 'm': 20.07507547878467, 'culture': 14.497956960497987, 'from': 3.4009076038439705, 'respiratory': 6.674586682713265, 'specimen': 5.390665693294843, 'january': 9.35876211116004, 'through': 5.92443184555309, 'december': 4.406102542241235, 'were': 13.780377832076736, 'identified': 6.115524326946596, 'microbiology': 7.76153613959767, 'record': 5.5751478271758, 'reviewed': 4.938129135820298, 'result': 3.068843046070576, 'whom': 6.296013743579654, 'required': 3.893819145979954, 'admission': 5.071589714738473, 'most': 18.063812695281005, 'communityacquired': 7.397329423575854, 'affected': 4.228394264395966, 'all': 2.1187611479693014, 'age': 3.4335617407351897, 'group': 2.893090186424186, 'but': 6.617080582547733, 'wa': 3.7739297607303763, 'common': 19.721553130033804, 'in': 1.6692199099011285, 'infant': 11.874408069536946, 'preschool': 20.574293011650862, 'child': 8.414103634304498, 'it': 4.646699023489005, 'occurred': 4.568328258369484, 'yearround': 11.085102729880786, 'fall': 6.996675842459666, 'spring': 7.702184005104275, 'more': 6.291965840836243, 'than': 4.581373727061359, 'threequarters': 11.22260625363072, 'had': 14.008230067548661, 'comorbidities': 22.676675791505247, 'twentyfour': 9.578750063855995, 'isolates': 6.102109155186476, 'associated': 2.597765114673604, 'pneumonia': 27.1512802413202, 'upper': 10.618835268243666, 'tract': 9.034638926716497, 'bronchiolitis': 15.407512849846057, 'cough': 5.288915598678487, 'fever': 4.375705560456732, 'malaise': 9.048679321630912, 'symptom': 6.820857896232547, 'crepitation': 27.474358852920957, 'wheeze': 10.364625258503148, 'sign': 5.084334453458499, 'only': 5.40240422460101, 'bronchial': 7.835450535967738, 'breathing': 7.1684089592419085, 'immunocompromised': 6.851047391018757, 'likely': 3.9772912432386436, 'nonimmunocompromised': 12.085102729880786, 'to': 2.7549315354364436, 'present': 2.7751613655470515, 'versus': 5.2977937500249395, 'p': 3.720968074872734, 'uneventful': 9.555849661745917, 'recovery': 5.336604903006417, 'recovered': 5.93120529414703, 'following': 3.688206092254851, 'some': 2.836905599168126, 'complication': 3.9843209476465673, 'died': 16.579002509114574, 'because': 3.603211584980148, 'due': 2.9690541909932566, 'underlying': 4.850394144441652, 'who': 2.7978625823505365, 'other': 2.101396537221436, 'conclusion': 2.2644885872176985, 'our': 2.381199156436122, 'similar': 3.5697612806287413, 'published': 8.977148228131579, 'data': 2.341339311136819, 'except': 6.129320749752797, 'for': 0.6182615215591839, 'finding': 3.2678283286256513, 'that': 2.0486293970770206, 'mortality': 6.690184087103194, 'rate': 2.8038576057275333, 'high': 2.6335637700659325, 'is': 4.733733697429346, 'a': 0.8665324072214281, 'cause': 6.03350385228575, 'lower': 3.7012021123005057, 'remains': 4.371013356491117, 'one': 4.761690733660349, 'frequent': 5.4307921824688945, 'atypical': 6.740403327334765, 'particularly': 4.208864861571978, 'among': 2.8553553301987695, 'young': 5.411071001994645, 'adult': 4.24790487423258, 'although': 3.199614332569365, 'highly': 3.628281649705873, 'transmissible': 6.5445343485180825, 'caused': 9.886040420876993, 'by': 3.1686232903655416, 'organism': 23.002098813988443, 'are': 1.1853681875232742, 'relatively': 4.884984351638213, 'minor': 6.648201944608141, 'include': 3.8894509033201037, 'pharyngitis': 9.64971658521014, 'tracheobronchitis': 11.315715658022203, 'croup': 10.296606835074497, 'fifth': 8.38971623946598, 'being': 3.5688654749241513, 'asymptomatic': 5.109124525354762, 'infected': 3.2322895185847376, 'subject': 4.8872158989367955, 'develop': 4.173983154243742, 'consistent': 5.446831384993429, 'bronchopneumonia': 10.143654912235899, 'rare': 5.784646165296247, 'fastidious': 11.02097239246107, 'difficult': 4.8399822270558035, 'grow': 7.625671111243489, 'on': 1.0372507120153864, 'therefore': 3.3977270464433182, 'diagnosis': 3.716199487556267, 'usually': 4.987070646920259, 'confirmed': 3.676387009567011, 'serological': 6.511111346980632, 'test': 3.561006277352989, 'or': 1.3915761732586722, 'polymerase': 4.84063077507342, 'chain': 4.55833676131181, 'reactiongene': 17.544534348518084, 'amplification': 6.381513708937289, 'technique': 3.898088142363349, 'kauh': 16.544534348518084, 'facility': 5.572990794567311, 'perform': 5.652750645299772, 'ha': 1.5321280460037334, 'been': 1.7274757534523857, 'available': 3.1590763473301147, 'since': 3.333482597336483, 'information': 3.4487252801850996, 'concerning': 6.232218601827976, 'scarce': 7.120368059699983, 'we': 1.3344718652778513, 'wished': 10.959571847796926, 'study': 1.4739311883324122})\n",
            "--- 32.19731688499451 seconds to parse query including caculate IDF over corpus---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "6JXs7IzJd1nA",
        "outputId": "86ec4b91-d3ef-413c-8739-07541fa9f5d1"
      },
      "source": [
        "# 8 mins\n",
        "keys = list(idf_matrix.keys())\n",
        "values = [float(idf_matrix[k]) for k in keys]\n",
        "pd.Series(values).describe()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    404445.000000\n",
              "mean         16.408394\n",
              "std           1.989375\n",
              "min           0.290668\n",
              "25%          15.959572\n",
              "50%          17.544534\n",
              "75%          17.544534\n",
              "max          17.544534\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTMe9eDWMJ33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a33e38e2-863e-4e77-bcf3-a5bfe98f396a"
      },
      "source": [
        "#######################################\n",
        "# Main Function (part 1)\n",
        "# run shorter queries\n",
        "# (3)caculate cos-similarity\n",
        "#######################################\n",
        "##########\n",
        "# caculate cos-similarity\n",
        "# run program\n",
        "start7= time.time()\n",
        "cos_M_covid_key = cos_sim(cord_file,covid_query_key_weight,covid_weight,dict_output,covid_inverted_file,length_covid, 1)\n",
        "# save cos-smilarity matrix\n",
        "with open('gdrive/My Drive/covid_key_cos_similairy.txt', 'w') as f: \n",
        "    f.write(json.dumps(cos_M_covid_key))\n",
        "print(\"--- %s seconds to caculate cos similarity for covid dataset with short query---\"% (time.time() - start7))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 611.5406761169434 seconds to caculate cos similarity for covid dataset with short query---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6DRkbwsYvht"
      },
      "source": [
        "#######################################\n",
        "# Main Function (part 1)\n",
        "# run shorter queries\n",
        "# (4) create ranked list file\n",
        "#######################################\n",
        "rank_file_key = rank_list(cos_M_covid_key )\n",
        "#rank_file_full = rank_list(cos_M_covid_key_full )\n",
        "# save rank_file\n",
        "#with open('gdrive/My Drive/covid_rank_file_full.txt', 'w') as f: \n",
        "    #f.write(json.dumps(cos_M_covid_key_full))\n",
        "with open('gdrive/My Drive/covid_rank_file_key.txt', 'w') as f: \n",
        "  for i in range(len(rank_file_key)):\n",
        "    f.write(\"\\t\".join(str(x) for x in rank_file_key[i]) + '\\n')"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXdPTvxQ1c-v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d4b8dda6-79d4-4c69-eebf-123b094b3f37"
      },
      "source": [
        "#######################################\n",
        "# Main Function (part 2)\n",
        "# run longer queries\n",
        "# (1)create index\n",
        "#######################################\n",
        "\n",
        "########################\n",
        "# create index for query(full)\n",
        "start_norm = time.time()\n",
        "normal_query_key_full = readFile(cord_qes) #nomarlized query dataset\n",
        "print(\"--- %s seconds to normalize query key(full)----\"% (time.time() - start_norm))\n",
        "\n",
        "#posting list\n",
        "start0 = time.time()\n",
        "postlist_query_key_result_full = posting_list(normal_query_key_full) \n",
        "print(\"--- %s seconds to buid posting list for query key(full)----\"% (time.time() - start0))\n",
        "\n",
        "#dictionary\n",
        "start1 = time.time()\n",
        "dict_query_key_output_full = dictionary(postlist_query_key_result_full) \n",
        "print(\"cord_key (full) dataset has \" + str(len(dict_query_key_output_full)) + \" unique vocabulary set\")\n",
        "print(\"--- %s seconds to buid dictionary for query key(full)----\"% (time.time() - start1))\n",
        "#save dictionary\n",
        "with open('gdrive/My Drive/dictionary_covid_query_key_full.txt', 'w') as f: \n",
        "    f.write(json.dumps(dict_query_key_output_full))\n",
        "\n",
        "#inverted file\n",
        "start2 = time.time()\n",
        "inverted_query_key_full = inverted_file(dict_query_key_output_full.keys(),postlist_query_key_result_full)\n",
        "print(\"--- %s seconds to buid inverted file for query key(full)----\"% (time.time() - start2))\n",
        "# save inverted file\n",
        "with open(\"gdrive/My Drive/inverted_file_covid_query_key_full.bin\", \"wb\") as fb:\n",
        "  for num in inverted_query_key_full:\n",
        "    fb.write(num.to_bytes(4, \"big\"))\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 0.013148784637451172 seconds to normalize query key(full)----\n",
            "--- 0.0007472038269042969 seconds to buid posting list for query key(full)----\n",
            "cord_key (full) dataset has 223 unique vocabulary set\n",
            "--- 0.0004363059997558594 seconds to buid dictionary for query key(full)----\n",
            "--- 0.00040221214294433594 seconds to buid inverted file for query key(full)----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "gqTpvxjQfnKu",
        "outputId": "9a58bc5b-46a1-4580-b37d-e66301ff21b0"
      },
      "source": [
        "#######################################\n",
        "# Main Function (part 2)\n",
        "# run longer queries\n",
        "# (2)caculate weighting\n",
        "#######################################\n",
        "\n",
        "# query(long)\n",
        "idf_matrix = idf_corpus(dict_output,cord_file)\n",
        "covid_query_key_weight_full = tf_idf(normal_query_key_full,idf_matrix)\n",
        "length_covid_key_query_full = length_M(covid_query_key_weight_full)\n",
        "# TF-IDF for first query \n",
        "print(\"weight(TF-IDF) for first query of covid_key dataset:\", \"\\n\", covid_query_key_weight_full[0])\n",
        "print(\"--- %s seconds to parse query including caculate IDF over corpus---\"% (time.time() - start6 ))\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weight(TF-IDF) for first query of covid_key dataset: \n",
            " defaultdict(<class 'float'>, {'what': 4.535405558978827, 'is': 0.7889556162382244, 'the': 0.2906684016422432, 'origin': 5.312113421342007, 'of': 0.3000032691840576})\n",
            "--- 1832.507230758667 seconds to parse query including caculate IDF over corpus---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "dOLH4lvgPIPn",
        "outputId": "d2a8bf71-f2d1-42e6-9af6-6ccac03ca549"
      },
      "source": [
        "#######################################\n",
        "# Main Function (part 2)\n",
        "# run longer queries\n",
        "# (3)caculate cos-similarity\n",
        "#######################################\n",
        "\n",
        "##########\n",
        "# caculate cos-similarity\n",
        "# run program\n",
        "start7= time.time()\n",
        "cos_M_covid_key_full = cos_sim(cord_file,covid_query_key_weight_full,covid_weight,dict_output,covid_inverted_file,length_covid,1)\n",
        "# save cos-smilarity matrix\n",
        "with open('gdrive/My Drive/covid_full_cos_similairy.txt', 'w') as f: \n",
        "    f.write(json.dumps(cos_M_covid_key_full))\n",
        "print(\"--- %s seconds to caculate cos similarity for covid dataset with full query---\"% (time.time() - start7))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 617.6173405647278 seconds to caculate cos similarity for covid dataset with full query---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FO7HGvfNbHIv"
      },
      "source": [
        "#######################################\n",
        "# Main Function (part 1)\n",
        "# run longer queries\n",
        "# (4) create ranked list file\n",
        "#######################################\n",
        "\n",
        "rank_file_full = rank_list(cos_M_covid_key_full )\n",
        "# save rank_file\n",
        "with open('gdrive/My Drive/covid_rank_file_full.txt', 'w') as f: \n",
        "  for i in range(len(rank_file_full)):\n",
        "    f.write(\"\\t\".join(str(x) for x in rank_file_full[i]) + '\\n')"
      ],
      "execution_count": 34,
      "outputs": []
    }
  ]
}