{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR Programming Assignment #2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMCrzxHGFsLK"
      },
      "source": [
        "**Informaiton Retrieval Programming Assignment #2**\n",
        "<br>**Real-world Indexing System**\n",
        "<br>Build an inverted file that contains a postings list for each dictionary item.\n",
        "<br>- Inverted file is written to disk as a binary file\n",
        "<br>- Dictionary is also written to disk, which is in the form of a serialized object\n",
        "<br>- For each word in the dictionary, a file offset to the corresponding on-disk posting list is stored (word as well)\n",
        "<br>- Process the source text file only once\n",
        "<br>- This program follows the memory-based inversion algorithm (Algorithm A)\n",
        "\n",
        "\n",
        "<br><br>**Author:** Helen Ting He; **Date:** Sept 20, 2021"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxDIcpNKdv_1",
        "outputId": "12ce57be-ab59-4c8e-a231-4538ccd157d3"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from collections import Counter, defaultdict, OrderedDict\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "\n",
        "nltk.download('punkt')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RqYAkIvd-UB"
      },
      "source": [
        "# read original header file\n",
        "header_url = 'http://pmcnamee.net/744/data/headlines.txt'\n",
        "header = pd.read_csv(header_url,sep='\\n',header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_CkhBoa0ocX"
      },
      "source": [
        "# Test files with 500 and 3000 paragraphs \n",
        "with open('Favorites.txt') as f:\n",
        "    lines = f.readlines()\n",
        "testfile = pd.DataFrame(list(zip(lines)))\n",
        "with open('3000.txt') as f:\n",
        "    lines2 = list(line for line in (l.strip() for l in f) if line)\n",
        "testfile = pd.DataFrame(list(zip(lines)))\n",
        "testfile2 = pd.DataFrame(list(zip(lines2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iLWZdXNosLH"
      },
      "source": [
        "def normalization(each_para):\n",
        "  # To normalize each paragraph\n",
        "  # @input: each phragraph, string\n",
        "  # @output: counter of word for each paragraph, dict\n",
        "  # normalized method:\n",
        "  # 1. lower case \n",
        "  # 5. remove punctuation\n",
        "  # 4. remove non-alpha letters (vocabulary freq:462,313)\n",
        "  # 3. transform word numberals into numbers, then remove non-alpha again\n",
        "  # 2. substitution of contractions\n",
        "  # 6. remove single letter (vocabulary freq:157,136)\n",
        "  # 7. remove any words contain * sign (vocabulary freq:156,954)\n",
        "  # 8. remove any words contain + sign (vocabulary freq:156,776)\n",
        "  # 9. remove any words starting or ending with - sign and ,sign (vocabulary freq:155,854)\n",
        "  # 10. remove any words starting or ending with ..sign (vocabulary freq: 155,121)\n",
        "  # 11. remove any words starting or ending with :sign, =sign, _sign,`sign,^^sign (vocabulary freq: 154,838)\n",
        "  output_list_para = []\n",
        "  counter_list_para = {}\n",
        "  each_para = each_para.lower()\n",
        "  each_para = each_para.replace(\"'re\",' are').replace(\"'s\",' is').replace(\"n't\",' not')\n",
        "  each_para = each_para.replace(' one',' 1').replace(' two',' 2').replace(' three',' 3').replace(' four',' 4').replace(' five ',' 5').replace(' six ',' 6').replace(' seven ',' 7').replace(' eight ',' 8').replace(' nine ',' 9 ').replace(' ten ',' 9 ')\n",
        "  each_para = each_para.replace(\"'\",'')\n",
        "  for word in word_tokenize(each_para):\n",
        "    if(re.search(\"\\d\",word)):\n",
        "      continue\n",
        "    if(re.search(r'\\*', word)):\n",
        "      continue\n",
        "    if(re.search(r'\\+', word)):\n",
        "      continue\n",
        "    if(re.search(r'^-', word) or re.search(r'-$', word) or re.search(r'^,', word) or re.search(r',$', word)) :\n",
        "      continue\n",
        "    if(re.search(r'^\\..', word) or re.search(r'\\..$', word)) :\n",
        "      continue\n",
        "    if(re.search(r'^:', word) or re.search(r':$', word) or re.search(r'^=', word) or re.search(r'=$', word) or re.search(r'^_', word) or re.search(r'_$', word) or re.search(r'^`', word) or re.search(r'`$', word) or re.search(r'^\\^^', word) or re.search(r'\\^^$', word)) :\n",
        "      continue\n",
        "    if(len(word)!= 1 ):\n",
        "      output_list_para.append(word)   \n",
        "      counter_list_para = Counter(output_list_para)\n",
        "  return counter_list_para"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuA7mTVGrcUj"
      },
      "source": [
        "def readFile(whole_text): \n",
        "  ###############\n",
        "  # MAIN METHOD1\n",
        "  ###############\n",
        "  # To read the text and process relevant methods\n",
        "  # @input: input raw text, dataframe\n",
        "  # @output: paragraph one by one, string\n",
        "  i = 0\n",
        "  input_line = []\n",
        "  len_file = len(whole_text)\n",
        "  output_wordlist_para = []\n",
        "  output_wordlist_dict = {}\n",
        "  while (1+3*i) < len_file:\n",
        "    para = whole_text[0][1+3*i]\n",
        "    output_wordlist_dict[i] = normalization(para)\n",
        "    i = i + 1\n",
        "  return output_wordlist_dict\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0y31KoPOlaYh",
        "outputId": "541811f8-533f-4c6b-9c53-3bb6770d678d"
      },
      "source": [
        "start = time.time()\n",
        "normal_header = readFile(header)\n",
        "print(\"--- %s seconds to buid normnalized counter ----\"% (time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 187.51851320266724 seconds to buid normnalized counter ----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uB0zYurC7RDN",
        "outputId": "9746f59c-31e8-4a35-af84-ce709a60d661"
      },
      "source": [
        "# test print for docid = 0\n",
        "normal_header[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'breakfast': 1,\n",
              "         'club': 1,\n",
              "         'for': 1,\n",
              "         'gives': 1,\n",
              "         'hunger': 1,\n",
              "         'its': 1,\n",
              "         'marching': 1,\n",
              "         'orders': 1,\n",
              "         'veterans': 1,\n",
              "         'worcester': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XELoEXYR7J8R"
      },
      "source": [
        "def posting_list(docid_counter):\n",
        "  # convert into posting list\n",
        "  # @input:dict{ [docid]:dict{[word]: word freq per document} }, dict of dict\n",
        "  # @ouput:dict{ [word]:dict{[docid]: term count} }, dict of dict\n",
        "  posting_list_output = defaultdict(lambda: Counter([]))\n",
        "  for doc, cnt in docid_counter.items():\n",
        "    for word, word_cnt in cnt.items():\n",
        "      posting_list_output[word][doc] += word_cnt\n",
        "  return posting_list_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opZ1uF05WLKG",
        "outputId": "d790ab2b-a635-464d-9ade-430fcc57e6e8"
      },
      "source": [
        "start = time.time()\n",
        "posting_list_result = posting_list(normal_header)\n",
        "print(\"--- %s seconds to buid posting list----\"% (time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 7.137878179550171 seconds to buid posting list----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGz-AUVYHyqu",
        "outputId": "0b7e17eb-00df-4cfa-f1d5-ebfd100c54e7"
      },
      "source": [
        "posting_list_result['heidelberg']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({114329: 1,\n",
              "         135133: 1,\n",
              "         174780: 1,\n",
              "         221099: 1,\n",
              "         243837: 1,\n",
              "         452545: 1,\n",
              "         491139: 1,\n",
              "         491278: 1})"
            ]
          },
          "metadata": {},
          "execution_count": 354
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsT-uv1pzE58",
        "outputId": "aedc0eff-abb7-405e-b81b-38c5514fb3a3"
      },
      "source": [
        "# test return for 'daffodils' word\n",
        "posting_list_result['^^']\n",
        "posting_list_result['a-a']\n",
        "posting_list_result['daffodils']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({65342: 1, 267430: 1})"
            ]
          },
          "metadata": {},
          "execution_count": 269
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK2sdrU4ndCY"
      },
      "source": [
        "def dictionary(posting_list):\n",
        "  # To generate the unique sorted vocabulary list as key\n",
        "  # DF and sorted offset as value \n",
        "  # @input: dict{ [word]:dict{[docid]: term count} }, dict of dict\n",
        "  # @output: sorted unique vocabulary dict with DF and offset \n",
        "  sort_dict = {}\n",
        "  result_sort_dict = {}\n",
        "  offset_sum = 0\n",
        "  offset_i = 0\n",
        "  od = OrderedDict(sorted(posting_list.items()))\n",
        "  sort_dict = OrderedDict(od)\n",
        "  for i, word_i in enumerate(sort_dict.keys()):\n",
        "    offset_i = len(sort_dict[word_i]) * 2 #offset\n",
        "    result_sort_dict[word_i] = len(sort_dict[word_i].values()),offset_sum #DF\n",
        "    offset_sum = offset_sum + offset_i \n",
        "  return result_sort_dict\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3wcyE16rWrq",
        "outputId": "2b610f18-f49f-4c78-f5c1-c6557f979f2f"
      },
      "source": [
        "start = time.time()\n",
        "dict_output = dictionary(posting_list_result)\n",
        "print(\"--- %s seconds to buid dictionary----\"% (time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1.2549679279327393 seconds to buid dictionary----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45fZRrTAp16B"
      },
      "source": [
        "# save dictionary into disk\n",
        "# convert it into json\n",
        "with open('dictionary_header.txt', 'w') as f:\n",
        "    f.write(json.dumps(dictionary(posting_list_result)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V45AhG2QruLC"
      },
      "source": [
        "def inverted_file(key,posting_list):\n",
        "  # To generate inverted file\n",
        "  # @input: sorted key, list\n",
        "  #         posting list, dict of dict\n",
        "  # @output: inverted file, list\n",
        "  inverted_list = []\n",
        "  for word_i in key:\n",
        "    for docid, cnt in posting_list[word_i].items():\n",
        "      inverted_list.append(docid)\n",
        "      inverted_list.append(cnt)\n",
        "  return inverted_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCC_YDYAwfli"
      },
      "source": [
        "# save inverted file in binary into disk\n",
        "byte_file = inverted_file(dict_output.keys(),posting_list_result)\n",
        "with open(\"inverted_fiile_binary.bin\", \"wb\") as fb:\n",
        "  for num in byte_file:\n",
        "    fb.write(num.to_bytes(4, \"big\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Smq-NvR5CWSl"
      },
      "source": [
        "######## Testing ##########\n",
        "# read dictionary file \n",
        "with open (\"dictionary_header.txt\",\"r+\") as f:\n",
        "  data = f.read()\n",
        "dict_output = json.loads(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERQz_-21A_5W",
        "outputId": "9fb5b86f-645d-4df9-9039-aa8e246da738"
      },
      "source": [
        "# 1. Print out the document frequency and postings list for terms: “Heidelberg\".\n",
        "# Heidelberg: \n",
        "df = dict_output['Heidelberg'.lower()][0]\n",
        "print(\"document frequency for Heidelberg: \", df)\n",
        "posting_list_index = dict_output['Heidelberg'.lower()][1]\n",
        "# get index of Heidelberg at dictionary key posiotion\n",
        "index_heidelberg = list(dict_output.keys()).index('heidelberg')\n",
        "next_word = list(dict_output)[index_heidelberg + 1]\n",
        "# get index of posting list for next word\n",
        "next_word_index = dict_output[next_word][1]\n",
        "# index range of Heidelberg at inverted_file \n",
        "range_index = [posting_list_index, next_word_index]\n",
        "print(\"index of range for Heidelberg: \", range_index)\n",
        "# read inverted_file binary file every 4 byte\n",
        "# then get their posting list \n",
        "list_num = []\n",
        "with open(\"inverted_fiile_binary.bin\", \"br\") as bf:\n",
        "  for i in range(posting_list_index):\n",
        "    data = bf.read(4)\n",
        "  for i in range(posting_list_index, next_word_index):\n",
        "    data = bf.read(4)\n",
        "    number = int.from_bytes(data,\"big\")\n",
        "    list_num.append(number)\n",
        "print(\"posting list for Heidelberg: \")\n",
        "print(list_num)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "document frequency for Heidelberg:  8\n",
            "index of range for Heidelberg:  [3203092, 3203108]\n",
            "posting list for Heidelberg: \n",
            "[114329, 1, 135133, 1, 174780, 1, 221099, 1, 243837, 1, 452545, 1, 491139, 1, 491278, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yfc5CGQ-HdYs",
        "outputId": "73b2dd41-8e48-48ad-d9f1-af6336b957a7"
      },
      "source": [
        "#2. Give document frequency, but do not print postings for the words: \"Hopkins\", “Stanford\", \"Brown\", and “college”\n",
        "#(these postings lists are longer).\n",
        "#Hokpins\n",
        "df = dict_output['hopkins'][0]\n",
        "print(\"document frequency for Hokpins: \", df)\n",
        "#Stanford\n",
        "df = dict_output['stanford'][0]\n",
        "print(\"document frequency for Stanford: \", df)\n",
        "#Brown\n",
        "df = dict_output['brown'][0]\n",
        "print(\"document frequency for brown: \", df)\n",
        "#college\n",
        "df = dict_output['college'][0]\n",
        "print(\"document frequency for college: \", df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "document frequency for Hokpins:  71\n",
            "document frequency for Stanford:  150\n",
            "document frequency for brown:  770\n",
            "document frequency for college:  1910\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKXYpsktKdC0",
        "outputId": "3a74a08f-c3b9-4b43-e097-bc63ee3b9f90"
      },
      "source": [
        "#3. Print out the docids for documents that have both \"Elon\" and \"Musk\" in the text. \n",
        "#Elon\n",
        "posting_list_index = dict_output['Elon'.lower()][1]\n",
        "# get index of elon at dictionary key posiotion\n",
        "index_elon = list(dict_output.keys()).index('elon')\n",
        "next_word = list(dict_output)[index_elon + 1]\n",
        "# get index of posting list for next word\n",
        "next_word_index = dict_output[next_word][1]\n",
        "# index range of elonat inverted_file \n",
        "range_index = [posting_list_index, next_word_index]\n",
        "print(\"index of range for elon: \", range_index)\n",
        "# read inverted_file binary file every 4 byte\n",
        "# then get their posting list \n",
        "list_num = []\n",
        "with open(\"inverted_fiile_binary.bin\", \"br\") as bf:\n",
        "  for i in range(posting_list_index):\n",
        "    data = bf.read(4)\n",
        "  for i in range(posting_list_index, next_word_index):\n",
        "    data = bf.read(4)\n",
        "    number = int.from_bytes(data,\"big\")\n",
        "    list_num.append(number)\n",
        "print(\"posting list for elon: \")\n",
        "print(list_num)\n",
        "#Musk\n",
        "posting_list_index = dict_output['musk'.lower()][1]\n",
        "# get index of musk at dictionary key posiotion\n",
        "index_musk = list(dict_output.keys()).index('musk')\n",
        "next_word = list(dict_output)[index_musk + 1]\n",
        "# get index of posting list for next word\n",
        "next_word_index = dict_output[next_word][1]\n",
        "# index range of musk at inverted_file \n",
        "range_index = [posting_list_index, next_word_index]\n",
        "print(\"index of range for musk: \", range_index)\n",
        "# read inverted_file binary file every 4 byte\n",
        "# then get their posting list \n",
        "list_num2 = []\n",
        "with open(\"inverted_fiile_binary.bin\", \"br\") as bf:\n",
        "  for i in range(posting_list_index):\n",
        "    data = bf.read(4)\n",
        "  for i in range(posting_list_index, next_word_index):\n",
        "    data = bf.read(4)\n",
        "    number = int.from_bytes(data,\"big\")\n",
        "    list_num2.append(number)\n",
        "print(\"posting list for musk: \")\n",
        "print(list_num2)\n",
        "# find both elon and musk in the same file \n",
        "set_elon = set(list_num)\n",
        "set_musk = set(list_num2)\n",
        "print(\"find both elon and musk in the same file: \")\n",
        "setone = {1}\n",
        "set_same = set_elon.union(set_musk) - setone\n",
        "print(set_same)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "index of range for elon:  [2191074, 2191194]\n",
            "posting list for elon: \n",
            "[3393, 1, 16330, 1, 19262, 1, 21341, 1, 29749, 1, 39287, 1, 44321, 1, 45978, 1, 52990, 1, 57023, 1, 57787, 1, 71988, 1, 84806, 1, 87959, 1, 98830, 1, 103398, 1, 104204, 1, 115207, 1, 122603, 1, 127050, 1, 128662, 1, 131441, 1, 131448, 1, 131514, 1, 135942, 1, 146965, 1, 151171, 1, 159147, 1, 186107, 1, 194998, 1, 197341, 1, 239304, 1, 240040, 1, 245923, 1, 249585, 1, 251252, 1, 274393, 1, 277539, 1, 283098, 1, 297139, 1, 301627, 1, 303775, 1, 305183, 1, 306988, 1, 307162, 1, 341755, 1, 342182, 1, 354346, 1, 369772, 1, 383528, 1, 399001, 1, 399946, 1, 420082, 1, 431495, 1, 431739, 1, 449684, 1, 456443, 1, 461816, 1, 479190, 1, 482769, 1]\n",
            "index of range for musk:  [4788658, 4788764]\n",
            "posting list for musk: \n",
            "[3393, 1, 16330, 1, 19262, 1, 21341, 1, 29749, 1, 44321, 1, 45978, 1, 52990, 1, 57023, 1, 57787, 1, 84806, 1, 98830, 1, 115207, 1, 122603, 1, 127050, 1, 128662, 1, 131448, 1, 131514, 1, 146965, 1, 159147, 1, 186107, 1, 194998, 1, 197341, 1, 229771, 1, 239304, 1, 240040, 1, 245923, 1, 249585, 1, 252219, 1, 253923, 1, 260295, 1, 267866, 1, 274393, 1, 283098, 1, 297139, 1, 303775, 1, 305183, 1, 306988, 1, 341755, 1, 342182, 1, 354346, 1, 369772, 1, 383528, 1, 399001, 1, 399946, 1, 420082, 1, 431495, 1, 431739, 1, 437850, 1, 449684, 1, 455802, 1, 456443, 1, 482769, 1]\n",
            "find both elon and musk in the same file: \n",
            "{135942, 115207, 104204, 98830, 146965, 305183, 44321, 277539, 383528, 354346, 306988, 71988, 29749, 301627, 252219, 19262, 3393, 84806, 127050, 399946, 267866, 437850, 21341, 369772, 131441, 251252, 39287, 131448, 455802, 431739, 151171, 431495, 229771, 449684, 128662, 87959, 399001, 45978, 303775, 456443, 186107, 245923, 342182, 240040, 159147, 297139, 194998, 131514, 57787, 57023, 260295, 239304, 16330, 482769, 479190, 274393, 283098, 307162, 197341, 253923, 103398, 122603, 249585, 420082, 461816, 341755, 52990}\n"
          ]
        }
      ]
    }
  ]
}
